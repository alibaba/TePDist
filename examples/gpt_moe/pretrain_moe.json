{
    "preprocess_config": {
      "preprocess_batch_size": 256,
      "input_schema": "input_ids:int:1024,input_masks:int:1024",
      "first_sequence": "text"
    },
    "model_config": {
      "model_type": "gpt_moe",
      "pretrain_model_name_or_path": "./pretrain_data/",
      "task_types": "lm",
      "max_patch_position_embeddings": 16,
      "patch_feature_size": 2048,
      "vocab_size": 50257,
      "hidden_size": 768,
      "intermediate_size": 6144,
      "num_hidden_layers": 8,
      "max_position_embeddings": 1024,
      "num_attention_heads": 16,
      "attention_head_size": 48,
      "type_vocab_size": 2,
      "num_experts": 8,
      "moe_gating": "top_2",
      "second_expert_policy": "all",
      "second_expert_threshold": 1.0,
      "local_dispatch": true,
      "num_local_groups": 8,
      "expert_capacity_dim": 256,
      "min_expert_capacity": 4,
      "capacity_factor_train": 0.5,
      "capacity_factor_eval": 2.0,
      "activation_fn": "relu",
      "loss_coef": 1e-2,
      "switch_policy_train": "argmax",
      "switch_policy_eval": "argmax",
      "switch_dropout": 0.0,
      "attention_probs_dropout_prob": 0.0,
      "hidden_dropout_prob": 0.0,
      "is_training": true
    },
    "train_config": {
      "train_input_fp": "./pretrain_data/train_list_files",
      "train_batch_size": 1,
      "save_steps": 10000,
      "num_epochs": 1,
      "model_dir": "./pretrain_data/ckpt",
      "optimizer_config": {
        "optimizer": "adam",
        "weight_decay_ratio": 0.01,
        "warmup_ratio": 0.01,
        "learning_rate": 1e-4,
        "gradient_clip": "global",
        "clip_norm_value": 1.0
      }
    },
    "evaluate_config": {
      "eval_input_fp": null,
      "eval_batch_size": 1
    }
  }
